# MCBE AI Agent 配置

# 服务器配置
HOST=0.0.0.0
PORT=8080

# 认证配置
SECRET_KEY=change-me-in-production
WEBSOCKET_PASSWORD=123456

# DeepSeek 配置
DEEPSEEK_API_KEY=

# OpenAI 配置 (可选)
# OPENAI_API_KEY=your-api-key-here

# Anthropic 配置 (可选)
# ANTHROPIC_API_KEY=your-api-key-here

# 默认 LLM 提供商
DEFAULT_PROVIDER=deepseek

# Worker 配置
LLM_WORKER_COUNT=2
QUEUE_MAX_SIZE=100

# 流式输出配置
STREAM_SENTENCE_MODE=true  # true=开启流式响应并按句子分批输出，false=关闭流式响应并在完成后按句子分批输出

# LLM 预热配置
LLM_WARMUP_ENABLED=true  # 是否在启动时预热 LLM 模型

# 日志配置
LOG_LEVEL=INFO
ENABLE_FILE_LOGGING=true

# WebSocket 消息去重配置
DEDUP_EXTERNAL_MESSAGES=true  # 是否排除sender为"外部"且事件为PlayerMessage的重复消息

# 是否启用 WebSocket 原始日志
ENABLE_WS_RAW_LOG=false

# 是否启用 LLM 原始日志
ENABLE_LLM_RAW_LOG=false

# 开发模式 - 跳过身份验证，仅用于本地开发调试
DEV_MODE=false

# MCP 服务器配置
MCP_ENABLED=false  # 是否启用 MCP 服务器集成

# MCP 服务器配置 (JSON 格式)
# 支持两种格式：
# 1. 官方格式（推荐）:
# MCP_SERVERS='{"mcpServers": {"filesystem": {"command": "npx", "args": ["-y", "@modelcontextprotocol/server-filesystem", "/data"]}}}'
# 2. 简化格式:
# MCP_SERVERS='{"filesystem": {"command": "npx", "args": ["-y", "@modelcontextprotocol/server-filesystem", "/data"]}}'

# MCP 服务器类型说明：
# - stdio 模式：通过 command 和 args 启动本地进程
#   {"server-name": {"command": "npx", "args": ["-y", "package-name"], "env": {"DEBUG": "true"}, "timeout": 10}}
# - HTTP 模式：连接远程 MCP 服务器
#   {"server-name": {"url": "http://localhost:8000/mcp"}}
# - SSE 模式（已废弃）：URL 以 /sse 结尾
#   {"server-name": {"url": "http://localhost:3001/sse"}}